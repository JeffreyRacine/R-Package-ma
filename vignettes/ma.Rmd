---
author:
  - name: Jeffrey Racine
    affiliation: McMaster University
    address: >
      Department of Economics
      Kenneth Taylor Hall, Room 426 McMaster University
      1280 Main Street West
      Hamilton, Ontario, Canada, L8S 4M4
    email: \email{racinej@mcmaster.ca}
    url: https://socialsciences.mcmaster.ca/people/racinej
title:
  formatted: "Model Averaging - The R Package \\pkg{ma}"
  plain:     "Model Averaging - The ma Package"
  short:     "\\pkg{ma}: Model Averaging"
abstract: >
  There exist a number of \proglang{R} packages that perform model averaging (see \pkg{AICcmodavg}, \pkg{BMA}, \pkg{glmulti} and \pkg{MuMLn} by way of illustration). Though existing packages contain a rich set of features, they rely on the practitioner to supply the set of candidate models and they tend to focus on parameters common to all candidate models (i.e., coefficients on linear predictors). The \pkg{ma} package takes a somewhat different approach. Using heuristics that can be modified by the user, we strive to automatically generate a rich set of candidate models that are subsequently averaged, and we avoid focusing on parameters per se, instead focusing on conditional mean models, their derivatives, and nonparametric inferential procedures. Categorical predictors can be modelled in a variety of ways including a kernel-smoothed varying coefficient approach. The resulting models are competitive with fully nonparametric methods and attenuate bias arising from model mis-specification.
keywords:
  formatted: [parametric, weighted average, "\\proglang{R}"]
  plain:     [parametric, weighted average, R]
preamble: >
  \usepackage{amsmath}
output: rticles::jss_article
bibliography: [ma.bib]
biblio-style: jss
link-citations: yes
---

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{R Package ma}
-->

# Introduction

Practitioners frequently wrestle with issues surrounding model uncertainty, and model *selection* is perhaps one of the most widely used solutions to this problem. Model selection deals with model uncertainty by selecting one model from among a set of candidate models based on a selection criterion. Model selection has a long history, and a variety of methods have been proposed, each based on distinct estimation criteria. These include Akaike's *An Information Criterion* (AIC; @AKAIKE:1970, @AKAIKE:1973), Mallows' $C_p$ (@MALLOWS:1973), the *Bayesian Information Criterion* (BIC; @SCHWARZ:1978), *delete-one cross-validation* (@STONE:1974), *generalized cross-validation* (@CRAVEN_WAHBA:1979), and the *Focused Information Criterion* (FIC) (@CLAESKENS_HJORT:2003), to name but a few.

Model *averaging* deals with model uncertainty not by having the user select one model from among a set of candidate models according to a criterion such as $C_p$, AIC or BIC, but instead by averaging over the set of candidate models in a particular manner. The goal is to reduce estimation variance while controlling mis-specification bias. There is a longstanding literature on Bayesian model averaging; see @HOETING_MADIGAN_RAFTERY_VOLINSKY:1999 for a comprehensive review. There is also a rapidly-growing literature on frequentist methods for model averaging, including @BUCKLAND_BURNHAMN_AUGUSTIN:1997, @HANSEN:2007,
@WAN:2010, and @HANSEN_RACINE:2012, among others.

The \proglang{R} package \pkg{ma} aims to automate the process of model averaging thereby freeing the practitioner from tedious details.  The \pkg{ma} package adopts a frequentist approach; for those interested in a Bayesian approach see the \proglang{R} package \pkg{BMA}. It is assumed that the practitioner is using regression techniques and is concerned about the impact of model mis-specification on inference and prediction. As far as possible, the interface is designed to mimic that for \code{lm()} with some notable exceptions outlined below.

# Implementation of the Model Average Procedure

Consider a nonparametric regression model containing both categorical and continuous predictors where one is interested in modeling the unknown conditional mean in the following location-scale model,
\begin{equation}
  Y=g\left( \mathbf{X},\mathbf{Z}\right) +\sigma \left( \mathbf{X},\mathbf{Z}
  \right) \varepsilon ,  \label{MOD:csm}
\end{equation}
where $g(\mathbf{x},\mathbf{z})=E[Y|\mathbf{X}=\mathbf{x},\mathbf{Z}=\mathbf{z}]$ is an unknown function, $\mathbf{X=}\left(
  X_{1},\ldots ,X_{q}\right) ^{\mathrm{T}}$ is a $q$-dimensional
vector of continuous predictors, and $\mathbf{Z}=\left( Z_{1},\ldots
  ,Z_{r}\right) ^{\mathrm{T}}$ is an $r$-dimensional vector of
categorical predictors. Letting $\mathbf{z} =\left( z_{s}\right) _{s=1}^{r}$, we assume that $z_{s}$ takes $c_{s}$ different values in $D_{s}\equiv \left\{0,1,\ldots ,c_{s}-1\right\}$, $s=1,\ldots ,r$, and let $c_{s}$ be a finite positive constant. 

## Categorical Predictors and Candidate Model Selection

To handle the presence of categorical predictors, we might estimate $g(\cdot)$ by tensor-product polynomial splines weighted by categorical kernel functions. Let $\mathcal{B}\left( \mathbf{x}\right)$ be the tensor-product polynomial splines and $L\left(\mathbf{Z},\mathbf{z}, \mathbf{\lambda }\right)$ be a product categorical kernel function. Then the nonparametric function $g\left(\mathbf{x},\mathbf{z}\right)$ can be approximated by $\mathcal{B}\left( \mathbf{x}\right) ^{\mathrm{T}}\beta \left(\mathbf{z}\right)$, where $\beta\left( \mathbf{z}\right)$ is a $\mathbf{K}_{n}\times 1$ vector with $\mathbf{K}_{n}\rightarrow \infty$ as $n\rightarrow \infty$. We estimate $\beta \left(\mathbf{z}\right)$ by minimizing the following weighted least squares criterion,
\begin{equation}
  \widehat{\beta }\left( \mathbf{z}\right) =\arg \min_{\beta \in {R}^{\mathbf{K}_{n}}}\sum\limits_{i=1}^{n}\left\{ Y_{i}-
    \mathcal{B}\left( \mathbf{X}_{i}\right) ^{\mathrm{T}}\beta\right\} ^{2}L\left( \mathbf{Z}_{i},\mathbf{z},\mathbf{\lambda }
  \right) .  \label{EQ:betahatz}
\end{equation}
Thus $g\left( \mathbf{x},\mathbf{z}\right)$ is estimated by $\widehat{g}\left(\mathbf{x},\mathbf{z}\right)=\mathcal{B}\left(   \mathbf{x}\right)^{\mathrm{T}}\widehat{\beta}\left(\mathbf{z}\right)$. If $\mathcal{B}\left(   \mathbf{x}\right)=\mathbf{x}$ (i.e., the identity basis) the we are conducting standard linear (in predictors) weighted least squares estimation (i.e., $\widehat{g}\left(\mathbf{x},\mathbf{z}\right)= \mathbf{x}^{\mathrm{T}}\widehat{\beta}\left(\mathbf{z}\right)$). If in addition we introduced the categorical predictors as additive factors instead of using kernel weighting, we are conducting standard linear least squares with additive dummy variables (i.e., $\widehat{g}\left(\mathbf{x},\mathbf{z}\right)=\mathbf{z}^{\mathrm{T}}\widehat{\alpha}+\mathbf{x}^{\mathrm{T}}\widehat{\beta}$). Each of these possibilities can be entertained among the set of candidate models implemented in the \pkg{ma} package.

## Candidate Model Basis Function Choice

Note that $\mathcal{B}\left( \mathbf{x}\right)$ need not necessarily be a tensor product. It could be additive (semiparametric) or a
Taylor-type basis, perhaps augmented to allow for differing orders in each dimension. Regardless of the nature of the basis, each candidate model used in the model average will differ in terms of its spline degree in each dimension, number of knots, and smoothing parameters. Denote the $m$th such model as $\widehat{g}_m\left( \mathbf{x},\mathbf{z}\right)$. Having enumerated the $M$ candidate models over which the averaging is to take place, we will require a model averaging criterion. The \pkg{ma} package allows for two such criteria.

## Hansen's (2007) Mallows Model Average Criterion

The Mallows [@MALLOWS:1973] criterion for the model average estimator [@HANSEN:2007] is 
\begin{equation*}
  C_n(w)=w'\hat{\mathbf{E}}'\hat{\mathbf{E}}w+2\sigma^2K'w,
\end{equation*}
where $\hat{\mathbf{E}}$ is the $T\times M$ matrix with columns containing the
residual vector from the $m$th candidate estimating equation, $K$ the
$M\times 1$ vector of the number of parameters in each model, and
$\sigma^2$ the variance from the largest dimensional
model. This criterion is used to select
the weight vector $\hat w$, i.e.,
\begin{equation*}
  \hat w = \operatorname{argmin}_w C_n(w).
\end{equation*}
Because $\operatorname{argmin}_w C_n(w)$ has no closed-form solution, the weight vector is found numerically. The solution involves constrained minimization subject to non-negativity and summation constraints, which constitutes a classic quadratic programming problem. This criterion involves nothing more than computing the residuals for each candidate estimating equation, obtaining the rank of each candidate estimating equation, and solving a simple quadratic program. The Mallows model averaging (MMA) $C_n(w)$ criterion provides an estimate of the average squared error from the model average fit, and has been shown to be asymptotically optimal in the sense of achieving the lowest possible squared error in a class of model average estimators. See @HANSEN:2007 for further details. 

## Hansen and Racine's (2012) Jackknife Model Average Criterion

@HANSEN_RACINE:2012 propose an alternative jackknife model averaging (JMA) criterion for the model average estimator given by
\begin{equation*}
  CV_{n}(w)=\frac{1}{n}(y-\tilde{\mathbf{Y}}w)'(y-\tilde{\mathbf{Y}}w),
\end{equation*}
where $\tilde{\mathbf{Y}}$ is the $T\times M$ matrix with columns containing the jackknife estimator from the $m$th candidate estimating equation formed by deleting the $t$ observation when constructing the $t$th prediction. Like its Mallows counterpart, this involves solving a quadratic program where we minimize $(y-\tilde{\mathbf{Y}}w)'(y-\tilde{\mathbf{Y}}w)=y'y+w'\tilde{\mathbf{Y}}'\tilde{\mathbf{Y}}w-2y'\tilde{\mathbf{Y}}w$ and the first term is ignorable as it does not involve the weight vector $w$. In the presence of homoskedastic errors, JMA and MMA are nearly equivalent, but when the errors are heteroskedastic, JMA delivers models with significantly lower MSE.

Whether using JMA or MMA, both involve solving a simple quadratic program (the \proglang{R} package \pkg{quadprog} is invoked for its solution).

# R Function Interface and Defaults

The main function is called \code{lm.ma()}. Models are specified symbolically. A typical model has the form response ~ terms where response is the (numeric) response vector and terms is a series of terms which specifies a linear predictor for response. Note that, unlike \code{lm()} in which the formula interface specifies functional form, in \code{lm.ma()} the formula interface is strictly for listing the variables involved and the procedure will determine an appropriate model averaged functional form. Do not incorporate transformations, interactions and the like in the formula interface for lm.ma as these will most surely fail.

The function \code{lm.ma()} computes a model that is the weighted average of a set of least squares candidate models whose predictors are generated by common basis functions (additive, generalized Taylor polynomial, or tensor products). The candidate models increase in complexity from linear bases (if \code{degree.min=1}) through higher order ones up to degree.max. All bases are of the Bernstein polynomial class, as opposed to raw polynomials, and allow for differing degrees across multivariate predictors. When \code{knots=TRUE}, interior knots are used and the Bernstein polynomials become B-spline bases and we are then averaging over regression spline models. When the number of numeric predictors is two or more, the generalized Taylor polynomial includes interaction terms up to order degree minus one. Since we are averaging over models that are nonlinear in the predictors, derivatives will be vectors that potentially depend on the values of every predictor. An ad-hoc formula is used to determine the relationship between the largest (most complex) model, the sample size, and the number of predictors. This ad-hoc rule was set so that, as the sample size increases, we can approximate ever more complex functions while necessarily restricting the size of the largest model in small sample settings. Categorical predictors can enter additively and linearly (if \code{vc=FALSE}) or in a parsimonious manner by exploiting recent developments in semiparametric varying coefficient models along the lines of @LI_OUYANG_RACINE:2013. With the options \code{knots=TRUE} and \code{vc=TRUE}, we are averaging over varying-coefficient regression splines.

The heuristic used for \code{degree.max}, the maximum value for the basis degree in each dimension, defaults to \code{max(2,ceiling(log(n)-2*log(1+k)))} where \code{k} is the number of numeric predictors and \code{n} the number of observations. 

The heuristic used for \code{lambda.num.max}, the maximum value for the smoothing parameter grid in each dimension, defaults to \code{max(2,ceiling(log(n)-2*log(1+p)))} where \code{p} is the number of categorical predictors and \code{n} the number of observations.

## Generic Accessor Functions

The function summary is used to obtain and print a summary of the results. The generic accessor functions \code{coef}, \code{fitted}, \code{predict}, \code{plot} (see \code{?plot.lm} for details) and \code{residuals} extract various useful features of the value returned by lm.ma.

## Plotting lm.ma Objects.

The function \code{plot.lm.ma()} plots an \code{lm.ma} object. It can plot either the conditional mean (default) or partial derivatives of the conditional mean with respect to the predictors, whether of type \code{numeric} or \code{factor}. Nonparametric confidence intervals can be plotted via the option \code{plot.ci=TRUE}, while data (and a rug) can be included when \code{plot.data=TRUE} (\code{plot.rug=TRUE}).

# Illustration

By way of illustration, consider the following chunk of \proglang{R} code. We simulate data from a nonlinear data generating process (DGP). The practitioner might consider, say, linear regression via \code{lm(y~x)} but this model would clearly be mis-specified as might other models they consider. Instead, consider the model produced by \code{lm.ma(y~x)}. The fitted model and data are plotted in Figure \ref{sinfig}.

```{r}
library(ma)
set.seed(42)
n <- 1000
x <- runif(n)
dgp <- sin(4*pi*x)
y <- dgp + rnorm(n,sd=0.25*sd(dgp))
model <- lm.ma(y~x,verbose=FALSE)
```

\begin{figure}[!ht]
```{r,results='hide',echo=FALSE}
plot(model,plot.data=TRUE)
lines(x[order(x)],dgp[order(x)],lty=3,col=3)
abline(lm(y~x),col=2,lty=2)
legend("topright",c("Model Average","lm(y~x)","DGP"),col=1:3,lty=1:3,bty="n",cex=0.75)
```
\caption{\label{sinfig}Simulated illustration, $n=1000$ observations, model average estimate, linear (in predictor) estimate, and data generating process (DGP) (figure created via \code{plot(model,plot.data=TRUE)}).}
\end{figure}

Figure \ref{sinfig} reveals that the model average estimator is more faithful to the underlying DGP than the naive linear regression model. Of course no serious practitioner would likely have settled for the naive linear model, but that is besides the point. The point to be made is that the model average approach is known to possess a number of appealing characteristics, and this simple illustration a number of them.

# ANOVA-Based Inference

An ANOVA-based test of significance is considered. The standard asymptotics are not applicable in this setting (though they are available to the user). Instead a nonparametric bootstrap procedure is implemented.

The bootstrap procedure is straightforward. 

- Fit the model average model for all predictors (call this the
  unrestricted model).

- Fit the model average model for all predictors except the one
  whose significance is being tested (call this the restricted model).

- Compute the usual $F$-statistic from these two models. Call this
  statistic $F$.

- Use a residual-based bootstrap procedure based on the residuals
  from the restricted model, then bootstrap the $F$-statistic
  following the same steps as above. Call these
  $F^*_1,F^*_2,\dots,F^*_B$.

- Compute the nonparametric $P$-value in the usual manner (i.e.,
  $\hat P=B^{-1}\sum_{b=1}^B\mathbf{1}(F^*_b>F)$).

- Reject $H_0$ if $\hat P<\alpha$ where $\alpha$ is the nominal
  level of the test.
  
  
```{r}
model <- lm.ma(y~x,
               compute.anova=TRUE,
               compute.anova.boot=TRUE,
               degree.min=1,
               verbose=FALSE)
summary(model)
```

Extensive simulations reveal that the test is correctly sized and displays power that increases with the departure from the null and the sample size.

# Parallel Computation

Parallel processing can be enabled by adding the option \code{parallel=TRUE}. This can cut computation by up to \code{1/parallel.cores}, particularly for multiple predictor models where computation of each candidate model may be non-trivial. Note that the default number of cores invoked is the number of cores present in the CPU on which the code is executed. Note also that, when \code{parallel=TRUE}, this applies to all computation *except* for solving the quadratic program which invokes a call to  \code{solve.QP} from the \proglang{R} package \pkg{quadprog} that, at this time, does not support parallel processing.

# Limitations and Caveats

The model average approach implemented in the \pkg{ma} package is capable of statistically consistent estimation of certain classes of smooth functions (e.g., analytic) at rates approaching or equal to those of correctly specified parametric models (extensive simulations highlighting this feature are available upon request). In this sense the approach is competitive with and even more efficient than some popular nonparametric approaches such as locally weighted kernel estimation. This is achieved automatically and without input by the user. This stands in contrast to the model average approaches embodied in the \proglang{R} packages listed previously that rely on a set of candidate models provided by the user that may not be imbued with this feature *unless* the user similarly ties the dimensions of their candidate models to the available sample information, among other important aspects (such as including interactions among predictors and allowing for a sufficiently high polynomial order in the candidate models to capture a sufficiently rich set of nonlinearities).
 
It should come as no surprise that this approach, like its nonparametreic peers, will face the *curse of dimensionality*. To partially address this shortcoming, in order for our model average approach to deliever statistically consistent estimation of broad classes of functions, the complexity of the maximal basis function in the set of candidate models to be averaged over must increase with the amount of sample information present, among other important aspects. However, for a finite sample size, the maximum dimension of the basis is constrained by this limit. Some heuristics already mentioned seek to trade off consistency versus computational feasibility given the data at hand. Of course, default settings appropriate for one DGP may be inappropriate for another, so some sensitivity analysis may be called for to ensure that the final results are not upset by modifying the default configuration.

When using the default settings in high dimensional situations, the number of candidate models may grow unreasonably large (say 2,500 or more). Furthermore, the dimension of the maximal basis may similarly grow unreasonably large (say 5,000 or more). In such cases you might want to i) increase \code{S}, ii) increase \code{lambda.S} (if categorical predictors are present and \code{vc=TRUE}), iii) set and restrict \code{degree.max}, iv) set and restrict \code{lambda.num.max} if categorical predictors are present, v) reduce \code{segments.max} (if \code{knots=TRUE}),  vi) set \code{all.combinations=FALSE}, or perhaps instead consider a semiparametric model (\code{basis="additive"} and \code{vc=FALSE} produces semiparametric additive candidate models - see the example in \code{?india} for an illustration).

It is therefore prudent to recollect that the goal of model averaging is to outperform model assertion or model selection by reducing estimation variance while controlling mis-specification bias. The goal is not to obtain statistically consistent estimators - that remains the domain of fully nonparametric estimation.

# References {-}
