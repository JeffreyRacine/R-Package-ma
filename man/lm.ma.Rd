\name{lm.ma}
\alias{lm.ma}
\alias{lm.ma.default}
\alias{lm.ma.formula}
\title{
Fitting Model Average Models
}
\description{
A function with an interface similar to \code{\link{lm}} that averages over a set of linear (in parameters) candidate models.
}
\usage{
lm.ma(...)

\method{lm.ma}{default}(y = NULL,
      X = NULL,
      X.eval = NULL,
      basis = c("glp","tensor","additive"),
      compute.deriv = FALSE,
      deriv.order = 1,
      degree.max = NULL,
      segments.max = 3,
      knots = FALSE,
      S = 10,
      exhaustive = TRUE,
      method = c("jma","mma"),
      ma.weights = NULL,
      bootstrap.ci = FALSE,
      B = 199,
      alpha = 0.05,
      weights = NULL,
      vc = TRUE,
      verbose = TRUE,
      ...)

\method{lm.ma}{formula}(formula,
      data = list(),
      y = NULL,
      X = NULL,
      X.eval = NULL,
      basis = c("glp","tensor","additive"),
      compute.deriv = FALSE,
      deriv.order = 1,
      degree.max = NULL,
      segments.max = 3,
      knots = FALSE,
      S = 10,
      exhaustive = TRUE,
      method = c("jma","mma"),
      ma.weights = NULL,
      bootstrap.ci = FALSE,
      B = 199,
      alpha = 0.05,
      weights = NULL,
      vc = TRUE,
      verbose = TRUE,
      ...)

}

\arguments{
  \item{formula}{ a symbolic description of the model to be fit }

  \item{data}{ an optional data frame containing the variables in the model }

  \item{y}{a one dimensional vector of dependent data}

  \item{X}{a \eqn{p}-variate data frame of explanatory (training) data }

  \item{X.eval}{a \eqn{p}-variate data frame of points on which the regression will be estimated (evaluation data)}

  \item{basis}{ a character string indicating whether the generalized polynomial, additive or tensor product basis should be used}

  \item{compute.deriv}{a logical value indicating whether or not to compute derivatives}
  
  \item{deriv.order}{integer indicating order of derivative desired}

  \item{degree.max}{maximum value for the basis in each dimension (defaults to \code{max(2,round((S/num.x)*(n/100)^0.25)} where \code{num.x} is the number of numeric (i.e., non categorical) predictors and n the number of observations}

  \item{segments.max}{when \code{knots=TRUE} the maximum number of segments (i.e., interior knots plus 1) to use for the B-spline basis}
  
  \item{knots}{a logical value indicating whether to include interior knots or not (set equal to the degree)}

  \item{S}{the constant in the data-driven rule for determining degree.max}
  
  \item{exhaustive}{a logical value indicating whether to conduct exhaustive search or not}
  
  \item{method}{whether to use jackknife model averaging (jma) or Mallows model averaging (mma)}

  \item{ma.weights}{a vector of model average weights obtained from a previous invocation (useful for bootstrapping etc.)}
  
  \item{bootstrap.ci}{a logical value indicating whether to bootstrap nonparametric confidence intervals or not}
  
  \item{B}{the number of bootstrap replications desired}
  
  \item{alpha}{value in (0,1) used to compute \eqn{1-\alpha\%} confidence intervals}
  
  \item{weights}{an optional vector of weights to be used in the fitting process. Should be \code{NULL} or a numeric vector; if non-NULL, weighted least squares is used with weights \code{weights} (that is, minimizing \eqn{\sum_iw_ie_i^2}); otherwise ordinary least squares is used}

  \item{vc}{a logical value indicating whether to allow the categorical predictors to enter additively (only the intercept can shift) or to instead use a varying coefficient structure (all parameters can shift)}
  
  \item{verbose}{a logical value indicating whether to report detailed progress during computation}
  
  \item{...}{optional arguments to be passed}

}
\details{

Models for lm.ma are specified symbolically. A typical model has the form \code{response ~ terms} where response is the (numeric) response vector and terms is a series of terms which specifies a linear predictor for response. Typical usages are
\preformatted{
    model <- lm.ma(y~x1+x2)
    model <- lm.ma(y~x1+x2,compute.deriv=TRUE)
    model <- lm.ma(y~x1+x2,bootstrap.ci=TRUE)    
    summary(model)
    fitted(model)
    coef(model)
    predict(model,newdata=foo)
}

This function computes model averages over least squares candidate models whose predictors are generated by common basis functions (additive, multivariate polynomial, or tensor products). The candidate models increase in complexity from linear bases through higher order ones up to \code{degree.max}. For computational purposes,  you need not span all possible combinations of degrees (set \code{exhaustive=FALSE}). The default basis function (\code{basis="glp"}) uses a (multivariate) Taylor-type basis with Bernstein (as opposed to raw) polynomials. When \code{knots=TRUE} then interior knots are used and the Bernstein polynomials become B-spline bases and we are then averaging over regression spline models. For degrees higher than one, interaction terms are included up to order degree minus one. Since we are averaging over models that are nonlinear in the predictors, derivatives will be vectors that potentially depend on the values of all predictors. An ad-hoc formula is used to determine the relationship between the largest (most complex) model, the sample size, and the number of predictors. This was chosen so that as the sample size increases we can approximate ever more complex functions while we must necessarily restrict the size of the largest model in small sample settings. Categorical predictors can enter additively and linearly (\code{vc=FALSE}) or in a parsimonious manner by exploiting recent developments in semiparametric varying coefficient models along the lines of Li, Ouyang, and Racine (2013). With the options \code{knots=TRUE} and \code{vc=TRUE}, we are averaging over varying-coefficient regression splines.

This approach frees the user from using either model assertion or selection methods and thereby attenuates bias arising from model misspecification. Simulations reveal that this approach is competitive with the best existing semi- and nonparametric approaches. Because it uses only least squares fits, it is also fairly computationally efficient.

}
\value{

\code{lm.ma} returns an object of class "lm.ma".

The function summary is used to obtain and print a summary of the results. The generic accessor functions \code{coef}, \code{fitted}, \code{predict} and \code{residuals} extract various useful features of the value returned by lm.ma.

An object of class "lm.ma" is a list containing at least the following components:

  \item{fitted.values}{vector of fitted values}
  \item{deriv}{matrix of derivative vectors for each predictor}
  \item{ma.weights}{model average weights}
  \item{degree.max}{value of degree.max for each dimension (set by the default rule given above unless manually overridden)}
  \item{fitted.ci.l}{\eqn{\alpha/2} nonparametric confidence value for the fitted/predicted values}
  \item{fitted.ci.u}{\eqn{1-\alpha/2} nonparametric confidence value for the fitted/predicted values}  
  \item{deriv.ci.l}{\eqn{\alpha/2} nonparametric confidence value matrix for
    the derivatives}
  \item{deriv.ci.u}{\eqn{1-\alpha/2} nonparametric confidence value matrix for
    the derivatives}
  \item{r.squared}{appropriate measure of goodness of fit (Doksum and Samarov (1995))}
  \item{residuals}{model residuals}

}
\references{
  Doksum, K. and A. Samarov (1995), \dQuote{Nonparametric estimation of global functionals and a measure of the explanatory power of covariates in regression,} The Annals of Statistics, 23 1443-1473.  
  
  Li, Q. and D. Ouyang and J.S. Racine (2013), \dQuote{Categorical Semiparametric Varying Coefficient Models,} Journal of Applied Econometrics, Volume 28, 551-579.
}
\author{
Jeffrey S. Racine
}
\note{
This code is in beta status until further notice - proceed accordingly.
}

\seealso{
\code{\link{lm}}, \code{\link[crs]{crs}}, \code{\link[np]{npreg}}
}
\examples{
## Example 1

set.seed(42)
n <- 100
x <- sort(runif(n))
dgp <- cos(2*pi*x)
y <- dgp + rnorm(n,sd=0.5*sd(dgp))

model <- lm.ma(y~x,bootstrap.ci=TRUE,compute.deriv=TRUE)

summary(model)

par(mfrow=c(1,2))

plot(x,y,cex=0.25)
lines(x,fitted(model))
lines(x,model$fitted.ci.l,col=2,lty=2)
lines(x,model$fitted.ci.u,col=2,lty=2)

ylim <- range(c(model$deriv.ci.l[,1],model$deriv.ci.u[,1]))
plot(x,model$deriv[,1],ylim=ylim,ylab="First derivative",type="l")
lines(x,model$deriv.ci.l[,1],col=2,lty=2)
lines(x,model$deriv.ci.u[,1],col=2,lty=2)

par(mfrow=c(1,1))

## Example 2

require(rgl)

set.seed(42)
n <- 1000
x1 <- runif(n)
x2 <- runif(n)

dgp <- cos(2*pi*x1)*sin(2*pi*x2)

y <- dgp + rnorm(n,sd=0.5*sd(dgp))

n.eval <- 25
x.seq <- seq(0,1,length=n.eval)
newdata <- data.frame(expand.grid(x.seq,x.seq))
names(newdata) <- c("x1","x2")

model <- lm.ma(y~x1+x2)

summary(model)

z <- matrix(predict(model,newdata=newdata),n.eval,n.eval)

num.colors <- 1000
colorlut <- topo.colors(num.colors)
col <- colorlut[ (num.colors-1)*(z-min(z))/(max(z)-min(z)) + 1 ]

## Open an rgl 3d window and use `persp3d()', a high-level function
## for 3D surfaces (and define the size of the window to be
## 640x640). The function par3d() passes in a window size (the default
## is 256x256 which is quite small), the function rgl.viewpoint()
## allows you to modify the `field of view' to get more of a
## `perspective' feel to the plot, while the function grid3d() adds a
## grid to the plot.

open3d()

par3d(windowRect=c(900,100,900+640,100+640))
rgl.viewpoint(theta = 0, phi = -70, fov = 80)

persp3d(x.seq,x.seq,z=z,
        xlab="X1",ylab="X2",zlab="Y",
        ticktype="detailed",
        border="red",
        color=col,
        alpha=.7,
        back="lines",
        main="Conditional Mean")

grid3d(c("x", "y+", "z"))

## Example 3

require(crs)
data(wage1)

## Classical linear regression model

model.lm <- lm(lwage ~ female + married + educ + exper + tenure, 
               data = wage1)

summary(model.lm)

## Compare with the model average estimator (female and married are factors)

model <- lm.ma(lwage ~ female + married + educ + exper + tenure, 
               compute.deriv = TRUE, 
               data = wage1)

summary(model)

## Compare coefficients from the linear model with the values from model averaging 
## for the non-factor predictors

apply(coef(model),2,summary)
coef(model.lm)[4:6]

## Example 4

data(cps71)
attach(cps71)
model.ma <- lm.ma(logwage~age)
plot(age,logwage,cex=0.25)
lines(age,fitted(model.ma))
require(np)
model.kernel <- npreg(logwage~age,regtype="ll",bwmethod="cv.aic")
lines(age,fitted(model.kernel),col=2,lty=2)
model.spline <- crs(logwage~age,cv.threshold=0)
lines(age,fitted(model.spline),col=3,lty=3)
legend("topleft",c("Parametric Model Average",
                   "Nonparametric Kernel",
                   "Nonparametric B-Spline"),
                   col=1:3,
                   lty=1:3,
                   bty="n")
                   
summary(model.spline)
summary(model.kernel)
summary(model.ma)
}

\keyword{Regression}

